# project01-nishoof

| File          | Description                                                                                                                                                                                                                                                                                             |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `crawl.go`    | Crawls the provided href. First downloads the page. Then extracts the words and hrefs. Cleans the hrefs. Then repeats the crawling process using the new hrefs. Uses a queue and tracks visited URLs to avoid cycles. Returns a set of all unique words found and a set of the hrefs that were crawled. |
| `download.go` | Downloads the contents of a web page using HTTP and returns a readable stream for further processing.                                                                                                                                                                                                   |
| `extract.go`  | Extracts unique words and hrefs, skipping unwanted elements like `<style>` and `<script>`.                                                                                                                                                                                                              |
| `clean.go`    | Cleans/normalizes URLs and resolves relative links against a base URL. For example, if we are currently on `https://github.com/` and we have the href `/nishoof`, cleaning it would give `https://github.com/nishoof`.                                                                                  |
